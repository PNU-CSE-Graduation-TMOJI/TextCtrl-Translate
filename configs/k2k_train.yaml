# python -X utf8 train.py

data:
  target: "src.dataset.textdata.WrappedDataModule"
  batch_size: 2
  num_worksers: 8 # modified
  train:
    size: 256
    subset_ratio: 0.90  # 90%만 사용
    root_dir: "/home/undergrad/model_base/TextCtrl-Translate/Syn_data/train/"
    font_dir: "/home/undergrad/model_base/TextCtrl-Translate/Syn_data/fonts/"

  validation:
    size: 256
    root_dir: "/home/undergrad/model_base/TextCtrl-Translate/Syn_data/eval/"
    font_dir: "/home/undergrad/model_base/TextCtrl-Translate/Syn_data/fonts/"



model:
  target: "src.trainer.CtrlBase.ControlBase"
  params:
    control_config:
      target: "src.trainer.CtrlBase.StylePyramidNet"
      params:
        image_size: 256
        patch_size: 16
        in_channels: 3
        embed_dim: 768
        model_channels: 320
        channel_mult: [ 1, 2, 4, 4 ]
        pyramid_sizes: [ 32, 16, 8, 4]
        use_checkpoint: True


    base_config:
      noise_scheduler: "diffusers.PNDMScheduler"
      scheduler_config: "/home/undergrad/model_base/TextCtrl-Translate/weights/sd/scheduler/scheduler_config.json"
      num_inference_steps: 50
      min_training_steps: 0

      vae:
        pretrained: "/home/undergrad/model_base/TextCtrl-Translate/weights/sd/vae/"
        normalizer: 0.18215

      text_encoder_optimized: False
      text_encoder:
        target: "src.module.textencoder.modules.LabelEncoder"
        params:
          max_len: 24
          emb_dim: 768
          ckpt_path: "/home/undergrad/model_base/TextCtrl-Translate/weights/text_encoder.ckpt"

      unet_pretrained: "/home/undergrad/model_base/TextCtrl-Translate/weights/sd/unet/diffusion_pytorch_model.bin"
      unet:
        target: "src.trainer.CtrlBase.ControlUNetModel"
        params:
          cross_attention_dim: 768

      reconstruction_loss: True
      ocr_loss_alpha: 0.01
      cond_on_text_image: False
      font_path: "/home/undergrad/model_base/TextCtrl-Translate/Syn_data/fonts/Pretendard-Medium.ttf"

      ocr_model:
        # model_name_or_path: "/home/undergrad/model_base/TextCtrl-Translate/weights/trocr_model.bin"
        model_name_or_path: "/home/undergrad/model_base/ko_trocr_tr/output/fold4/"
        ocr_supervised: True
        optimize: false
        max_length: 256
        num_beams: 4
        # height: 32
        # width: 128
        # ocr_supervised: True
        # pretrained: ".../weights/ocr_model.pth"
        # optimize: false
        # max_length: 25
        # charset_path: ".../src/module/abinet/data/charset_36.txt"
        # iter_size: 3
        # ensemble: ''
        # use_vision: False
        # vision:
        #   checkpoint:
        #   loss_weight: 1.
        #   attention: 'position'
        #   backbone: 'transformer'
        #   backbone_ln: 3
        #   max_length: 25
        #   charset_path: ".../src/module/abinet/data/charset_36.txt"
        # language:
        #   checkpoint:
        #   num_layers: 4
        #   loss_weight: 1.
        #   detach: True
        #   use_self_attn: False
        #   max_length: 25
        #   charset_path: ".../src/module/abinet/data/charset_36.txt"
        # alignment:
        #   loss_weight: 1.
        #   max_length: 25
        #   charset_path: ".../src/module/abinet/data/charset_36.txt"

      vgg_weight: "/home/undergrad/model_base/TextCtrl-Translate/weights/vgg19.pth"

#####################################################Lightning##########################################
load_ckpt_path: "/home/undergrad/model_base/TextCtrl-Translate/logs/lightning_logs/version_17/checkpoints/epoch=29-step=9990.ckpt" # model ckpt

lightning:
  precision: 32
  log_every_n_steps: 32
  accumulate_grad_batches: 32
  max_epochs: 100 #100
  accelerator: "gpu"
  strategy: ddp # ddp, dp?
  default_root_dir: "./logs"
  devices: [4, 5, 6, 7]
  # [4, 5, 6, 7]
  # strategy: 
  #   class_path: pytorch_lightning.strategies.DDPStrategy
  #   init_args:
  #     gradient_as_bucket_view: false    # ← 추가 (stride 문제 해결)
  #     find_unused_parameters: false     # ← 추가 (성능 향상)
  #     static_graph: true                # ← 추가 (성능 향상)
  #     bucket_cap_mb: 25                 # ← 추가 (메모리 최적화)

image_logger:
  target: "src.trainer.Base.BaseImageLogger"
  params:
    train_batch_frequency: 2000
    valid_batch_frequency: 2
    disable_wandb: false
    generation_kwargs:
      num_inference_steps: 50
      num_sample_per_image: 1
      guidance_scale: 2
      seed: 42
